import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve, precision_recall_curve
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.combine import SMOTEENN
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'AppleGothic'
plt.rcParams['axes.unicode_minus'] = False

# í•©ì„± ì‹ ìš©ì¹´ë“œ ì‚¬ê¸° ë°ì´í„°ì…‹ ìƒì„±
print("\ní•©ì„± ì‹ ìš©ì¹´ë“œ ì‚¬ê¸° ë°ì´í„°ì…‹ ìƒì„± ì¤‘...")
np.random.seed(42)

# íŠ¹ì„± ìƒì„±
n_samples = 10000
n_features = 28  # V1-V28 (ìµëª…í™”ëœ íŠ¹ì„±)

# ì •ìƒ ê±°ë˜ ìƒì„± (í´ë˜ìŠ¤ 0)
normal_samples = int(n_samples * 0.995)  # 99.5% ì •ìƒ
fraud_samples = n_samples - normal_samples  # 0.5% ì‚¬ê¸°

# ì •ìƒ ê±°ë˜: ë‚®ì€ ê°’, ë” ì§‘ì¤‘ëœ ë¶„í¬
X_normal = np.random.normal(0, 1, (normal_samples, n_features))
# ì‚¬ê¸° ê±°ë˜: ë†’ì€ ê°’, ë” ë¶„ì‚°ëœ ë¶„í¬
X_fraud = np.random.normal(2, 2, (fraud_samples, n_features))

# ë°ì´í„° ê²°í•©
X = np.vstack([X_normal, X_fraud])
y = np.hstack([np.zeros(normal_samples), np.ones(fraud_samples)])

# ê±°ë˜ ê¸ˆì•¡ íŠ¹ì„± ì¶”ê°€ (ì •ìƒ: $10-1000, ì‚¬ê¸°: $1-10000)
amount_normal = np.random.uniform(10, 1000, normal_samples)
amount_fraud = np.random.uniform(1, 10000, fraud_samples)
amount = np.hstack([amount_normal, amount_fraud])

# ê¸ˆì•¡ì„ íŠ¹ì„±ìœ¼ë¡œ ì¶”ê°€
X = np.column_stack([X, amount])

print(f"ë°ì´í„°ì…‹ í˜•íƒœ: {X.shape}")
print(f"í´ë˜ìŠ¤ ë¶„í¬: {np.bincount(y.astype(int))}")
print(f"ì‚¬ê¸° ë¹„ìœ¨: {fraud_samples/n_samples*100:.2f}%")

# -----------------------
# ë°ì´í„° ì „ì²˜ë¦¬
# -----------------------
print("\nğŸ“˜ ë°ì´í„° ì „ì²˜ë¦¬")
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ë°ì´í„° ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42, stratify=y)
print(f"í›ˆë ¨ ì„¸íŠ¸: {X_train.shape}, í…ŒìŠ¤íŠ¸ ì„¸íŠ¸: {X_test.shape}")
print(f"í›ˆë ¨ ì‚¬ê¸° ë¹„ìœ¨: {np.mean(y_train)*100:.2f}%")

# -----------------------
# ê¸°ì¤€ ëª¨ë¸ (ë¶ˆê· í˜• ë°ì´í„°)
# -----------------------
print("\nğŸ“˜ ê¸°ì¤€ ëª¨ë¸ - ë¶ˆê· í˜• ë°ì´í„°")

models = {
    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),
    'LogisticRegression': LogisticRegression(random_state=42, max_iter=1000)
}

baseline_results = {}
for name, model in models.items():
    print(f"\n{name} í•™ìŠµ ì¤‘ (ê¸°ì¤€)...")
    
    # ëª¨ë¸ í•™ìŠµ
    model.fit(X_train, y_train)
    
    # ì˜ˆì¸¡
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1]
    
    # í‰ê°€ ì§€í‘œ
    acc = accuracy_score(y_test, y_pred)
    auc = roc_auc_score(y_test, y_pred_proba)
    
    baseline_results[name] = {
        'accuracy': acc,
        'auc': auc,
        'predictions': y_pred,
        'probabilities': y_pred_proba
    }
    
    print(f"{name} ì •í™•ë„: {acc:.3f}")
    print(f"{name} AUC: {auc:.3f}")

# -----------------------
# SMOTEë¡œ ê· í˜• ì¡íŒ ë°ì´í„°
# -----------------------
print("\nğŸ“˜ SMOTE - ê· í˜• ì¡íŒ ë°ì´í„°")
smote = SMOTE(random_state=42)
X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)

print(f"ê· í˜• ì¡íŒ í›ˆë ¨ ì„¸íŠ¸: {X_train_balanced.shape}")
print(f"ê· í˜• ì¡íŒ ì‚¬ê¸° ë¹„ìœ¨: {np.mean(y_train_balanced)*100:.2f}%")

# ê· í˜• ì¡íŒ ë°ì´í„°ë¡œ ëª¨ë¸ í•™ìŠµ
balanced_results = {}
for name, model in models.items():
    print(f"\n{name} í•™ìŠµ ì¤‘ (SMOTE ê· í˜•)...")
    
    # ìƒˆë¡œìš´ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    if name == 'RandomForest':
        balanced_model = RandomForestClassifier(n_estimators=100, random_state=42)
    else:
        balanced_model = LogisticRegression(random_state=42, max_iter=1000)
    
    # ëª¨ë¸ í•™ìŠµ
    balanced_model.fit(X_train_balanced, y_train_balanced)
    
    # ì˜ˆì¸¡
    y_pred = balanced_model.predict(X_test)
    y_pred_proba = balanced_model.predict_proba(X_test)[:, 1]
    
    # í‰ê°€ ì§€í‘œ
    acc = accuracy_score(y_test, y_pred)
    auc = roc_auc_score(y_test, y_pred_proba)
    
    balanced_results[name] = {
        'accuracy': acc,
        'auc': auc,
        'predictions': y_pred,
        'probabilities': y_pred_proba
    }
    
    print(f"{name} (SMOTE) ì •í™•ë„: {acc:.3f}")
    print(f"{name} (SMOTE) AUC: {auc:.3f}")

# -----------------------
# ë”¥ëŸ¬ë‹: ì‹ ê²½ë§
# -----------------------
print("\nğŸ“˜ ë”¥ëŸ¬ë‹ - ì‚¬ê¸° íƒì§€ë¥¼ ìœ„í•œ ì‹ ê²½ë§")

# PyTorch í…ì„œë¡œ ë³€í™˜
X_train_tensor = torch.FloatTensor(X_train_balanced)
y_train_tensor = torch.LongTensor(y_train_balanced)
X_test_tensor = torch.FloatTensor(X_test)
y_test_tensor = torch.LongTensor(y_test)

# ì‹ ê²½ë§ êµ¬ì¶•
class FraudNN(nn.Module):
    def __init__(self, input_size=29):
        super(FraudNN, self).__init__()
        self.fc1 = nn.Linear(input_size, 64)
        self.fc2 = nn.Linear(64, 32)
        self.fc3 = nn.Linear(32, 16)
        self.fc4 = nn.Linear(16, 2)  # 2ê°œ í´ë˜ìŠ¤: ì •ìƒ/ì‚¬ê¸°
        self.dropout = nn.Dropout(0.3)
        
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.dropout(x)
        x = torch.relu(self.fc2(x))
        x = self.dropout(x)
        x = torch.relu(self.fc3(x))
        x = self.fc4(x)
        return x

# ëª¨ë¸ ì´ˆê¸°í™”
nn_model = FraudNN()
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(nn_model.parameters(), lr=0.001)

# ì‹ ê²½ë§ í•™ìŠµ
print("ì‹ ê²½ë§ í•™ìŠµ ì¤‘...")
nn_model.train()
for epoch in range(50):
    optimizer.zero_grad()
    outputs = nn_model(X_train_tensor)
    loss = criterion(outputs, y_train_tensor)
    loss.backward()
    optimizer.step()
    
    if (epoch + 1) % 10 == 0:
        print(f'ì—í¬í¬ [{epoch+1}/50], ì†ì‹¤: {loss.item():.4f}')

# ì‹ ê²½ë§ í‰ê°€
nn_model.eval()
with torch.no_grad():
    outputs = nn_model(X_test_tensor)
    _, predicted = torch.max(outputs.data, 1)
    nn_accuracy = accuracy_score(y_test, predicted.numpy())
    nn_probabilities = torch.softmax(outputs, dim=1)[:, 1].numpy()
    nn_auc = roc_auc_score(y_test, nn_probabilities)
    
    balanced_results['NeuralNetwork'] = {
        'accuracy': nn_accuracy,
        'auc': nn_auc,
        'predictions': predicted.numpy(),
        'probabilities': nn_probabilities
    }
    
    print(f"ì‹ ê²½ë§ ì •í™•ë„: {nn_accuracy:.3f}")
    print(f"ì‹ ê²½ë§ AUC: {nn_auc:.3f}")

# -----------------------
# ì‹œê°í™”: ê²°ê³¼ ë¶„ì„
# -----------------------
print("\nğŸ“˜ ì‹œê°í™” - ê²°ê³¼ ë¶„ì„")

# 1. í´ë˜ìŠ¤ ë¶„í¬ ë¹„êµ
plt.figure(figsize=(15, 10))

# ì›ë³¸ vs ê· í˜• ì¡íŒ ë¶„í¬
plt.subplot(2, 3, 1)
original_dist = np.bincount(y_train.astype(int))
balanced_dist = np.bincount(y_train_balanced.astype(int))
x = np.arange(2)
width = 0.35

plt.bar(x - width/2, original_dist, width, label='ì›ë³¸', alpha=0.7)
plt.bar(x + width/2, balanced_dist, width, label='SMOTE ê· í˜•', alpha=0.7)
plt.xlabel('í´ë˜ìŠ¤')
plt.ylabel('ê°œìˆ˜')
plt.title('í´ë˜ìŠ¤ ë¶„í¬: ì›ë³¸ vs ê· í˜•')
plt.xticks(x, ['ì •ìƒ', 'ì‚¬ê¸°'])
plt.legend()

# 2. í˜¼ë™í–‰ë ¬ ë¹„êµ
for i, (name, result) in enumerate(balanced_results.items()):
    plt.subplot(2, 3, i + 2)
    cm = confusion_matrix(y_test, result['predictions'])
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f'{name} í˜¼ë™í–‰ë ¬')
    plt.xlabel('ì˜ˆì¸¡ê°’')
    plt.ylabel('ì‹¤ì œê°’')

plt.tight_layout()
plt.show()

# 3. ROC ê³¡ì„  ë¹„êµ
plt.figure(figsize=(12, 5))

# ê¸°ì¤€ vs ê· í˜• ROC ê³¡ì„ 
plt.subplot(1, 2, 1)
for name, result in baseline_results.items():
    fpr, tpr, _ = roc_curve(y_test, result['probabilities'])
    plt.plot(fpr, tpr, label=f'{name} ê¸°ì¤€ (AUC = {result["auc"]:.3f})')

plt.plot([0, 1], [0, 1], 'k--', label='ë¬´ì‘ìœ„')
plt.xlabel('ê±°ì§“ ì–‘ì„±ë¥ ')
plt.ylabel('ì§„ì§œ ì–‘ì„±ë¥ ')
plt.title('ROC ê³¡ì„ : ê¸°ì¤€ ëª¨ë¸')
plt.legend()
plt.grid(True)

# ê· í˜• ëª¨ë¸ ROC ê³¡ì„ 
plt.subplot(1, 2, 2)
for name, result in balanced_results.items():
    fpr, tpr, _ = roc_curve(y_test, result['probabilities'])
    plt.plot(fpr, tpr, label=f'{name} ê· í˜• (AUC = {result["auc"]:.3f})')

plt.plot([0, 1], [0, 1], 'k--', label='ë¬´ì‘ìœ„')
plt.xlabel('ê±°ì§“ ì–‘ì„±ë¥ ')
plt.ylabel('ì§„ì§œ ì–‘ì„±ë¥ ')
plt.title('ROC ê³¡ì„ : ê· í˜• ëª¨ë¸')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

# 4. ì •ë°€ë„-ì¬í˜„ìœ¨ ê³¡ì„ 
plt.figure(figsize=(10, 6))
for name, result in balanced_results.items():
    precision, recall, _ = precision_recall_curve(y_test, result['probabilities'])
    plt.plot(recall, precision, label=f'{name}')

plt.xlabel('ì¬í˜„ìœ¨')
plt.ylabel('ì •ë°€ë„')
plt.title('ì •ë°€ë„-ì¬í˜„ìœ¨ ê³¡ì„ ')
plt.legend()
plt.grid(True)
plt.show()

# -----------------------
# ëª¨ë¸ ë¹„êµ ë° ë¹„ì¦ˆë‹ˆìŠ¤ í†µì°°
# -----------------------
print("\nğŸ“˜ ëª¨ë¸ ë¹„êµ ë° ë¹„ì¦ˆë‹ˆìŠ¤ í†µì°°")
print("=" * 60)
print("ê¸°ì¤€ ëª¨ë¸ (ë¶ˆê· í˜• ë°ì´í„°):")
print("-" * 40)
for name, result in baseline_results.items():
    print(f"{name:15} | ì •í™•ë„: {result['accuracy']:.3f} | AUC: {result['auc']:.3f}")

print(f"\nê· í˜• ëª¨ë¸ (SMOTE):")
print("-" * 40)
for name, result in balanced_results.items():
    print(f"{name:15} | ì •í™•ë„: {result['accuracy']:.3f} | AUC: {result['auc']:.3f}")
print("=" * 60)

# ë¹„ì¦ˆë‹ˆìŠ¤ í•´ì„
print("\nğŸ“˜ ë¹„ì¦ˆë‹ˆìŠ¤ í•´ì„:")
print("- ì •ìƒ (0): ì •ìƒ ê±°ë˜ - ìŠ¹ì¸ ì²˜ë¦¬")
print("- ì‚¬ê¸° (1): ì‚¬ê¸° ê±°ë˜ - ê±°ë¶€ ì²˜ë¦¬")
print("- ê±°ì§“ ì–‘ì„±: ì •ìƒ ê±°ë˜ë¥¼ ì‚¬ê¸°ë¡œ ì˜ëª» ë¶„ë¥˜ (ê³ ê° ë¶ˆí¸)")
print("- ê±°ì§“ ìŒì„±: ì‚¬ê¸° ê±°ë˜ë¥¼ ì •ìƒìœ¼ë¡œ ì˜ëª» ë¶„ë¥˜ (ê¸ˆìœµ ì†ì‹¤)")

# ë¹„ìš© ë¶„ì„
print(f"\nğŸ“˜ ë¹„ìš© ë¶„ì„:")
print("- ì‚¬ê¸° íƒì§€ëŠ” ê¸ˆìœµ ë³´ì•ˆì— ë§¤ìš° ì¤‘ìš”")
print("- ê±°ì§“ ìŒì„±ì´ ê±°ì§“ ì–‘ì„±ë³´ë‹¤ ë” ë¹„ìš©ì´ í¼")
print("- SMOTEëŠ” ì†Œìˆ˜ í´ë˜ìŠ¤(ì‚¬ê¸°) íƒì§€ë¥¼ ê°œì„ ")
print("- ë†’ì€ AUCëŠ” ì¢‹ì€ íŒë³„ ëŠ¥ë ¥ì„ ë‚˜íƒ€ëƒ„")

# íŠ¹ì„± ì¤‘ìš”ë„ (ëœë¤ í¬ë ˆìŠ¤íŠ¸)
rf_model = models['RandomForest']
feature_names = [f'V{i+1}' for i in range(28)] + ['Amount']
feature_importance = pd.DataFrame({
    'feature': feature_names,
    'importance': rf_model.feature_importances_
}).sort_values('importance', ascending=False)

print(f"\nğŸ“˜ ìƒìœ„ 10ê°œ ì¤‘ìš” íŠ¹ì„±:")
for i, (_, row) in enumerate(feature_importance.head(10).iterrows()):
    print(f"{i+1:2d}. {row['feature']:8s} (ì¤‘ìš”ë„: {row['importance']:.3f})")

print("\nğŸ“˜ ë¹„ì¦ˆë‹ˆìŠ¤ ê¶Œì¥ì‚¬í•­:")
print("- ì‹¤ì‹œê°„ ì‚¬ê¸° íƒì§€ ì‹œìŠ¤í…œ êµ¬í˜„")
print("- ë” ë‚˜ì€ ì •í™•ë„ë¥¼ ìœ„í•´ ì•™ìƒë¸” ë°©ë²• ì‚¬ìš©")
print("- ê³ ê° ê²½í—˜ ìœ ì§€ë¥¼ ìœ„í•´ ê±°ì§“ ì–‘ì„±ë¥  ëª¨ë‹ˆí„°ë§")
print("- ìƒˆë¡œìš´ ì‚¬ê¸° íŒ¨í„´ìœ¼ë¡œ ì •ê¸°ì ì¸ ëª¨ë¸ ì¬í•™ìŠµ")
print("- ê±°ë˜ ê¸ˆì•¡ì„ ì¤‘ìš”í•œ íŠ¹ì„±ìœ¼ë¡œ ê³ ë ¤") 