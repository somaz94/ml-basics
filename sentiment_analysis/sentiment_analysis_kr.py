import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.pipeline import Pipeline
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from wordcloud import WordCloud
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'AppleGothic'
plt.rcParams['axes.unicode_minus'] = False

# í•„ìš”í•œ NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')

try:
    nltk.data.find('corpora/wordnet')
except LookupError:
    nltk.download('wordnet')

# í•©ì„± ê°ì • ë¶„ì„ ë°ì´í„°ì…‹ ìƒì„±
print("\ní•©ì„± ê°ì • ë¶„ì„ ë°ì´í„°ì…‹ ìƒì„± ì¤‘...")
np.random.seed(42)

# ê¸ì • ë° ë¶€ì • ë¦¬ë·° ìƒ˜í”Œ
positive_reviews = [
    "ì´ ì œí’ˆì€ ì •ë§ ë†€ë¼ì›Œìš”! ë„ˆë¬´ ì¢‹ì•„ìš”.",
    "í›Œë¥­í•œ í’ˆì§ˆê³¼ ì¢‹ì€ ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.",
    "ë©‹ì§„ ê²½í—˜, ê°•ë ¥íˆ ì¶”ì²œí•©ë‹ˆë‹¤!",
    "í™˜ìƒì ì¸ ì œí’ˆ, ê¸°ëŒ€ë¥¼ ë›°ì–´ë„˜ì—ˆì–´ìš”.",
    "ê°€ê²© ëŒ€ë¹„ í›Œë¥­í•œ ê°€ì¹˜, ë§¤ìš° ë§Œì¡±í•©ë‹ˆë‹¤.",
    "ë›°ì–´ë‚œ ì„±ëŠ¥ê³¼ ì‹ ë¢°ì„±ì…ë‹ˆë‹¤.",
    "ìµœê³  í’ˆì§ˆ, ìµœê³ ì˜ êµ¬ë§¤ì˜€ì–´ìš”!",
    "ë¯¿ì„ ìˆ˜ ì—†ëŠ” ê¸°ëŠ¥ë“¤, ì •ë§ ì‚¬ë‘í•´ìš”.",
    "ì œ í•„ìš”ì— ì™„ë²½í•˜ê³ , í›Œë¥­í•œ ì„ íƒì…ë‹ˆë‹¤.",
    "í›Œë¥­í•œ ë””ìì¸ê³¼ ê¸°ëŠ¥ì„±ì…ë‹ˆë‹¤."
]

negative_reviews = [
    "ë”ì°í•œ ì œí’ˆ, ëˆ ë‚­ë¹„ì…ë‹ˆë‹¤.",
    "ë”ì°í•œ í’ˆì§ˆ, ë§¤ìš° ì‹¤ë§í–ˆìŠµë‹ˆë‹¤.",
    "ë”ì°í•œ ê²½í—˜, ì¶”ì²œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.",
    "ë‚˜ìœ ì„œë¹„ìŠ¤ì™€ ì•ˆ ì¢‹ì€ ì œí’ˆì…ë‹ˆë‹¤.",
    "ìµœì•…ì˜ êµ¬ë§¤, ì´ê²ƒì€ í”¼í•˜ì„¸ìš”.",
    "ì‹¤ë§ìŠ¤ëŸ¬ìš´ í’ˆì§ˆ, ê°€ì¹˜ê°€ ì—†ì–´ìš”.",
    "ë‚˜ìœ ë””ìì¸ê³¼ ë¶€ì¡±í•œ ê¸°ëŠ¥ì„±ì…ë‹ˆë‹¤.",
    "ë”ì°í•œ ê³ ê° ì„œë¹„ìŠ¤ ê²½í—˜ì…ë‹ˆë‹¤.",
    "ì“¸ëª¨ì—†ëŠ” ì œí’ˆ, êµ¬ë§¤ë¥¼ í›„íšŒí•©ë‹ˆë‹¤.",
    "ë¶€ì¡±í•œ ì„±ëŠ¥ê³¼ ì‹ ë¢°ì„± ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤."
]

# ë³€í˜•ì„ ì¶”ê°€í•˜ì—¬ ë” ë§ì€ ìƒ˜í”Œ ìƒì„±
def generate_reviews(base_reviews, sentiment, count=500):
    reviews = []
    for _ in range(count):
        base = np.random.choice(base_reviews)
        # ë³€í˜• ì¶”ê°€
        variations = [
            "ì •ë§ " + base,
            base + " í™•ì‹¤íˆ!",
            "ì œ ìƒê°ì—ëŠ” " + base,
            base + " ì œ ì˜ê²¬ìœ¼ë¡œëŠ”.",
            "ì ˆëŒ€ì ìœ¼ë¡œ " + base,
            base + " ì˜ì‹¬ì˜ ì—¬ì§€ê°€ ì—†ì–´ìš”.",
            "ë¶„ëª…íˆ " + base,
            base + " ì˜ë¬¸ì˜ ì—¬ì§€ ì—†ì´.",
            "ëª…ë°±íˆ " + base,
            base + " í™•ì‹¤í•´ìš”."
        ]
        reviews.append(np.random.choice(variations))
    return reviews

# ë°ì´í„°ì…‹ ìƒì„±
positive_samples = generate_reviews(positive_reviews, 1, 500)
negative_samples = generate_reviews(negative_reviews, 0, 500)

# ë°ì´í„° ê²°í•©
texts = positive_samples + negative_samples
labels = [1] * 500 + [0] * 500

# DataFrame ìƒì„±
df = pd.DataFrame({
    'text': texts,
    'sentiment': labels
})

print(f"ë°ì´í„°ì…‹ í˜•íƒœ: {df.shape}")
print(f"ê°ì • ë¶„í¬: {df['sentiment'].value_counts()}")

# -----------------------
# í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
# -----------------------
print("\nğŸ“˜ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬")

def preprocess_text(text):
    # ì†Œë¬¸ì ë³€í™˜ (í•œê¸€ì€ ëŒ€ì†Œë¬¸ì êµ¬ë¶„ì´ ì—†ìœ¼ë¯€ë¡œ ìƒëµ)
    # íŠ¹ìˆ˜ë¬¸ìì™€ ìˆ«ì ì œê±° (í•œê¸€, ì˜ë¬¸, ê³µë°±ë§Œ ìœ ì§€)
    text = re.sub(r'[^ê°€-í£a-zA-Z\s]', '', text)
    # ì¶”ê°€ ê³µë°± ì œê±°
    text = ' '.join(text.split())
    return text

# ì „ì²˜ë¦¬ ì ìš©
df['processed_text'] = df['text'].apply(preprocess_text)

# ë°ì´í„° ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(
    df['processed_text'], df['sentiment'], 
    test_size=0.3, random_state=42, stratify=df['sentiment']
)

print(f"í›ˆë ¨ ì„¸íŠ¸: {len(X_train)}, í…ŒìŠ¤íŠ¸ ì„¸íŠ¸: {len(X_test)}")

# -----------------------
# TF-IDF ë²¡í„°í™”
# -----------------------
print("\nğŸ“˜ TF-IDF ë²¡í„°í™”")

# TF-IDF ë²¡í„°ë¼ì´ì € ìƒì„±
tfidf_vectorizer = TfidfVectorizer(
    max_features=1000,
    ngram_range=(1, 2),
    stop_words=None,  # í•œê¸€ ë¶ˆìš©ì–´ëŠ” ë³„ë„ë¡œ ì„¤ì • í•„ìš”
    min_df=2,
    max_df=0.95
)

# í›ˆë ¨ ë°ì´í„°ì— ë§ì¶”ê³  ë³€í™˜
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
X_test_tfidf = tfidf_vectorizer.transform(X_test)

print(f"TF-IDF íŠ¹ì„±: {X_train_tfidf.shape[1]}")

# -----------------------
# ì „í†µì  ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸
# -----------------------
print("\nğŸ“˜ ì „í†µì  ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸")

models = {
    'NaiveBayes': MultinomialNB(),
    'SVM': SVC(kernel='linear', random_state=42)
}

results = {}
for name, model in models.items():
    print(f"\n{name} í•™ìŠµ ì¤‘...")
    
    # ëª¨ë¸ í•™ìŠµ
    model.fit(X_train_tfidf, y_train)
    
    # ì˜ˆì¸¡
    y_pred = model.predict(X_test_tfidf)
    
    # í‰ê°€ ì§€í‘œ
    acc = accuracy_score(y_test, y_pred)
    
    results[name] = {
        'accuracy': acc,
        'predictions': y_pred,
        'model': model
    }
    
    print(f"{name} ì •í™•ë„: {acc:.3f}")

# -----------------------
# ë”¥ëŸ¬ë‹: ì‹ ê²½ë§
# -----------------------
print("\nğŸ“˜ ë”¥ëŸ¬ë‹ - ê°ì • ë¶„ì„ì„ ìœ„í•œ ì‹ ê²½ë§")

# PyTorchë¥¼ ìœ„í•´ í¬ì†Œ í–‰ë ¬ì„ ë°€ì§‘ í–‰ë ¬ë¡œ ë³€í™˜
X_train_dense = X_train_tfidf.toarray()
X_test_dense = X_test_tfidf.toarray()

# ì‹¤ì œ íŠ¹ì„± ìˆ˜ ê°€ì ¸ì˜¤ê¸°
input_size = X_train_dense.shape[1]
print(f"ì‹ ê²½ë§ ì…ë ¥ í¬ê¸°: {input_size}")

# PyTorch í…ì„œë¡œ ë³€í™˜
X_train_tensor = torch.FloatTensor(X_train_dense)
y_train_tensor = torch.LongTensor(y_train.values)
X_test_tensor = torch.FloatTensor(X_test_dense)
y_test_tensor = torch.LongTensor(y_test.values)

# ì‹ ê²½ë§ êµ¬ì¶•
class SentimentNN(nn.Module):
    def __init__(self, input_size):
        super(SentimentNN, self).__init__()
        self.fc1 = nn.Linear(input_size, 256)
        self.fc2 = nn.Linear(256, 128)
        self.fc3 = nn.Linear(128, 64)
        self.fc4 = nn.Linear(64, 2)  # 2ê°œ í´ë˜ìŠ¤: ë¶€ì •/ê¸ì •
        self.dropout = nn.Dropout(0.3)
        
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.dropout(x)
        x = torch.relu(self.fc2(x))
        x = self.dropout(x)
        x = torch.relu(self.fc3(x))
        x = self.fc4(x)
        return x

# ì‹¤ì œ ì…ë ¥ í¬ê¸°ë¡œ ëª¨ë¸ ì´ˆê¸°í™”
nn_model = SentimentNN(input_size)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(nn_model.parameters(), lr=0.001)

# ì‹ ê²½ë§ í•™ìŠµ
print("ì‹ ê²½ë§ í•™ìŠµ ì¤‘...")
nn_model.train()
for epoch in range(50):
    optimizer.zero_grad()
    outputs = nn_model(X_train_tensor)
    loss = criterion(outputs, y_train_tensor)
    loss.backward()
    optimizer.step()
    
    if (epoch + 1) % 10 == 0:
        print(f'ì—í¬í¬ [{epoch+1}/50], ì†ì‹¤: {loss.item():.4f}')

# ì‹ ê²½ë§ í‰ê°€
nn_model.eval()
with torch.no_grad():
    outputs = nn_model(X_test_tensor)
    _, predicted = torch.max(outputs.data, 1)
    nn_accuracy = accuracy_score(y_test, predicted.numpy())
    
    results['NeuralNetwork'] = {
        'accuracy': nn_accuracy,
        'predictions': predicted.numpy()
    }
    
    print(f"ì‹ ê²½ë§ ì •í™•ë„: {nn_accuracy:.3f}")

# -----------------------
# ì‹œê°í™”: ê²°ê³¼ ë¶„ì„
# -----------------------
print("\nğŸ“˜ ì‹œê°í™” - ê²°ê³¼ ë¶„ì„")

# 1. ëª¨ë¸ ë¹„êµ
plt.figure(figsize=(15, 10))

# ì •í™•ë„ ë¹„êµ
plt.subplot(2, 3, 1)
model_names = list(results.keys())
accuracies = [results[name]['accuracy'] for name in model_names]
colors = ['skyblue', 'lightcoral', 'lightgreen']

plt.bar(model_names, accuracies, color=colors)
plt.ylabel('ì •í™•ë„')
plt.title('ëª¨ë¸ ì •í™•ë„ ë¹„êµ')
plt.ylim(0, 1)
for i, v in enumerate(accuracies):
    plt.text(i, v + 0.01, f'{v:.3f}', ha='center')

# 2. í˜¼ë™í–‰ë ¬
for i, (name, result) in enumerate(results.items()):
    plt.subplot(2, 3, i + 2)
    cm = confusion_matrix(y_test, result['predictions'])
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f'{name} í˜¼ë™í–‰ë ¬')
    plt.xlabel('ì˜ˆì¸¡ê°’')
    plt.ylabel('ì‹¤ì œê°’')

plt.tight_layout()
plt.show()

# 3. ê¸ì • ë° ë¶€ì • ê°ì •ì— ëŒ€í•œ ì›Œë“œí´ë¼ìš°ë“œ
plt.figure(figsize=(12, 5))

# ê¸ì • ê°ì • ì›Œë“œí´ë¼ìš°ë“œ
plt.subplot(1, 2, 1)
positive_text = ' '.join(df[df['sentiment'] == 1]['processed_text'])
wordcloud_positive = WordCloud(
    font_path='/System/Library/Fonts/AppleSDGothicNeo.ttc',  # í•œê¸€ í°íŠ¸ ê²½ë¡œ ì¶”ê°€
    width=400, height=300, 
    background_color='white',
    colormap='Greens',
    max_words=100
).generate(positive_text)

plt.imshow(wordcloud_positive, interpolation='bilinear')
plt.axis('off')
plt.title('ê¸ì • ê°ì • ì›Œë“œí´ë¼ìš°ë“œ')

# ë¶€ì • ê°ì • ì›Œë“œí´ë¼ìš°ë“œ
plt.subplot(1, 2, 2)
negative_text = ' '.join(df[df['sentiment'] == 0]['processed_text'])
wordcloud_negative = WordCloud(
    font_path='/System/Library/Fonts/AppleSDGothicNeo.ttc',  # í•œê¸€ í°íŠ¸ ê²½ë¡œ ì¶”ê°€
    width=400, height=300, 
    background_color='white',
    colormap='Reds',
    max_words=100
).generate(negative_text)

plt.imshow(wordcloud_negative, interpolation='bilinear')
plt.axis('off')
plt.title('ë¶€ì • ê°ì • ì›Œë“œí´ë¼ìš°ë“œ')

plt.tight_layout()
plt.show()

# 4. íŠ¹ì„± ì¤‘ìš”ë„ (TF-IDF ì ìˆ˜)
plt.figure(figsize=(12, 6))

# íŠ¹ì„± ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
feature_names = tfidf_vectorizer.get_feature_names_out()

# ê° í´ë˜ìŠ¤ì— ëŒ€í•œ í‰ê·  TF-IDF ì ìˆ˜ ê³„ì‚°
positive_indices = df[df['sentiment'] == 1].index
negative_indices = df[df['sentiment'] == 0].index

# ëª¨ë“  ë°ì´í„°ì— ëŒ€í•œ TF-IDF í–‰ë ¬ ê°€ì ¸ì˜¤ê¸°
all_tfidf = tfidf_vectorizer.transform(df['processed_text'])

# í‰ê·  ì ìˆ˜ ê³„ì‚°
positive_scores = all_tfidf[positive_indices].mean(axis=0).A1
negative_scores = all_tfidf[negative_indices].mean(axis=0).A1

# ê° í´ë˜ìŠ¤ì˜ ìƒìœ„ íŠ¹ì„± ê°€ì ¸ì˜¤ê¸°
top_positive_features = np.argsort(positive_scores)[-10:]
top_negative_features = np.argsort(negative_scores)[-10:]

# ìƒìœ„ ê¸ì • íŠ¹ì„± í”Œë¡¯
plt.subplot(1, 2, 1)
top_positive_words = [feature_names[i] for i in top_positive_features]
top_positive_values = [positive_scores[i] for i in top_positive_features]

plt.barh(range(len(top_positive_words)), top_positive_values, color='green', alpha=0.7)
plt.yticks(range(len(top_positive_words)), top_positive_words)
plt.xlabel('í‰ê·  TF-IDF ì ìˆ˜')
plt.title('ìƒìœ„ ê¸ì • ê°ì • ë‹¨ì–´')
plt.gca().invert_yaxis()

# ìƒìœ„ ë¶€ì • íŠ¹ì„± í”Œë¡¯
plt.subplot(1, 2, 2)
top_negative_words = [feature_names[i] for i in top_negative_features]
top_negative_values = [negative_scores[i] for i in top_negative_features]

plt.barh(range(len(top_negative_words)), top_negative_values, color='red', alpha=0.7)
plt.yticks(range(len(top_negative_words)), top_negative_words)
plt.xlabel('í‰ê·  TF-IDF ì ìˆ˜')
plt.title('ìƒìœ„ ë¶€ì • ê°ì • ë‹¨ì–´')
plt.gca().invert_yaxis()

plt.tight_layout()
plt.show()

# -----------------------
# ëª¨ë¸ ë¹„êµ ë° NLP í†µì°°
# -----------------------
print("\nğŸ“˜ ëª¨ë¸ ë¹„êµ ë° NLP í†µì°°")
print("=" * 50)
print("ëª¨ë¸ ì„±ëŠ¥ ìš”ì•½:")
print("-" * 30)
for name, result in results.items():
    print(f"{name:15} | ì •í™•ë„: {result['accuracy']:.3f}")
print("=" * 50)

# NLP í•´ì„
print("\nğŸ“˜ NLP í•´ì„:")
print("- TF-IDF: íŠ¹ì„± ì¶”ì¶œì„ ìœ„í•œ ìš©ì–´ ë¹ˆë„-ì—­ë¬¸ì„œ ë¹ˆë„")
print("- Naive Bayes: ë² ì´ì¦ˆ ì •ë¦¬ì— ê¸°ë°˜í•œ í™•ë¥ ì  ë¶„ë¥˜ê¸°")
print("- SVM: í…ìŠ¤íŠ¸ ë¶„ë¥˜ë¥¼ ìœ„í•œ ì„œí¬íŠ¸ ë²¡í„° ë¨¸ì‹ ")
print("- ì‹ ê²½ë§: ê°ì • ë¶„ì„ì„ ìœ„í•œ ë”¥ëŸ¬ë‹ ì ‘ê·¼ë²•")

# íŠ¹ì„± í†µì°°
print(f"\nğŸ“˜ ì£¼ìš” íŠ¹ì„± (ìƒìœ„ 5ê°œ ê¸ì • ë‹¨ì–´):")
for i, word in enumerate(top_positive_words[-5:], 1):
    print(f"{i}. {word}")

print(f"\nğŸ“˜ ì£¼ìš” íŠ¹ì„± (ìƒìœ„ 5ê°œ ë¶€ì • ë‹¨ì–´):")
for i, word in enumerate(top_negative_words[-5:], 1):
    print(f"{i}. {word}")

print("\nğŸ“˜ NLP ëª¨ë²” ì‚¬ë¡€:")
print("- í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬: ì†Œë¬¸ì ë³€í™˜, íŠ¹ìˆ˜ë¬¸ì ì œê±°")
print("- TF-IDF ë²¡í„°í™”: ë‹¨ì–´ ì¤‘ìš”ë„ í¬ì°©")
print("- ë¶ˆìš©ì–´ ì œê±°: ì˜ë¯¸ ìˆëŠ” ë‹¨ì–´ì— ì§‘ì¤‘")
print("- N-gram íŠ¹ì„±: ë‹¨ì–´ ì¡°í•© í¬ì°©")
print("- êµì°¨ ê²€ì¦: ëª¨ë¸ ì¼ë°˜í™” ë³´ì¥")

# ìƒ˜í”Œ ì˜ˆì¸¡
print("\nğŸ“˜ ìƒ˜í”Œ ì˜ˆì¸¡:")
sample_texts = [
    "ì´ ì œí’ˆì€ ì •ë§ í™˜ìƒì ì…ë‹ˆë‹¤!",
    "ë”ì°í•œ í’ˆì§ˆ, ë§¤ìš° ì‹¤ë§í–ˆìŠµë‹ˆë‹¤.",
    "ì¢‹ì§€ë§Œ ë” ë‚˜ì„ ìˆ˜ ìˆì–´ìš”.",
    "ë†€ë¼ìš´ ê²½í—˜, ê°•ë ¥íˆ ì¶”ì²œí•©ë‹ˆë‹¤!"
]

for text in sample_texts:
    processed = preprocess_text(text)
    tfidf_features = tfidf_vectorizer.transform([processed])
    
    # ëª¨ë“  ëª¨ë¸ì—ì„œ ì˜ˆì¸¡ ê°€ì ¸ì˜¤ê¸°
    predictions = {}
    for name, result in results.items():
        if name == 'NeuralNetwork':
            with torch.no_grad():
                # ë°€ì§‘ ë°°ì—´ë¡œ ë³€í™˜í•˜ê³  ì˜¬ë°”ë¥¸ í˜•íƒœ ë³´ì¥
                nn_features = tfidf_features.toarray()
                nn_output = nn_model(torch.FloatTensor(nn_features))
                _, pred = torch.max(nn_output, 1)
                predictions[name] = pred.item()
        else:
            predictions[name] = result['model'].predict(tfidf_features)[0]
    
    sentiment = "ê¸ì •" if predictions['NaiveBayes'] == 1 else "ë¶€ì •"
    print(f"í…ìŠ¤íŠ¸: '{text}'")
    print(f"ê°ì •: {sentiment}")
    print(f"ëª¨ë¸ ì˜ˆì¸¡: {predictions}")
    print("-" * 40) 