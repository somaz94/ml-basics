import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.cluster import KMeans
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve
from sklearn.preprocessing import StandardScaler
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'AppleGothic'
plt.rcParams['axes.unicode_minus'] = False

# ë°ì´í„° ë¡œë“œ (ìœ ë°©ì•” ìœ„ìŠ¤ì½˜ì‹  ë°ì´í„°ì…‹)
print("\nìœ ë°©ì•” ìœ„ìŠ¤ì½˜ì‹  ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘...")
cancer = load_breast_cancer()
X = cancer.data
y = cancer.target

print(f"ë°ì´í„° í˜•íƒœ: {X.shape}, íƒ€ê²Ÿ í˜•íƒœ: {y.shape}")
print(f"íƒ€ê²Ÿ ë¶„í¬: {np.bincount(y)}")
print(f"íƒ€ê²Ÿ ì´ë¦„: {cancer.target_names}")
print(f"íŠ¹ì„± ì´ë¦„: {cancer.feature_names[:5]}...")  # ì²˜ìŒ 5ê°œ íŠ¹ì„±ë§Œ í‘œì‹œ

# -----------------------
# ë°ì´í„° ì „ì²˜ë¦¬
# -----------------------
print("\nğŸ“˜ ë°ì´í„° ì „ì²˜ë¦¬")
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ë°ì´í„° ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42, stratify=y)
print(f"í›ˆë ¨ ì„¸íŠ¸: {X_train.shape}, í…ŒìŠ¤íŠ¸ ì„¸íŠ¸: {X_test.shape}")

# -----------------------
# ì§€ë„í•™ìŠµ: ë¡œì§€ìŠ¤í‹± íšŒê·€ì™€ ëœë¤ í¬ë ˆìŠ¤íŠ¸
# -----------------------
print("\nğŸ“˜ ì§€ë„í•™ìŠµ - ìœ ë°©ì•” ë¶„ë¥˜")

models = {
    'LogisticRegression': LogisticRegression(random_state=42, max_iter=1000),
    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42)
}

results = {}
for name, model in models.items():
    print(f"\n{name} í•™ìŠµ ì¤‘...")
    
    # ëª¨ë¸ í•™ìŠµ
    model.fit(X_train, y_train)
    
    # ì˜ˆì¸¡
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None
    
    # í‰ê°€ ì§€í‘œ
    acc = accuracy_score(y_test, y_pred)
    auc = roc_auc_score(y_test, y_pred_proba) if y_pred_proba is not None else None
    
    results[name] = {
        'accuracy': acc,
        'auc': auc,
        'predictions': y_pred,
        'probabilities': y_pred_proba
    }
    
    print(f"{name} ì •í™•ë„: {acc:.3f}")
    if auc:
        print(f"{name} AUC: {auc:.3f}")

# -----------------------
# ë”¥ëŸ¬ë‹: ì‹ ê²½ë§
# -----------------------
print("\nğŸ“˜ ë”¥ëŸ¬ë‹ - ì•” ë¶„ë¥˜ë¥¼ ìœ„í•œ ì‹ ê²½ë§")

# PyTorch í…ì„œë¡œ ë³€í™˜
X_train_tensor = torch.FloatTensor(X_train)
y_train_tensor = torch.LongTensor(y_train)
X_test_tensor = torch.FloatTensor(X_test)
y_test_tensor = torch.LongTensor(y_test)

# ì‹ ê²½ë§ êµ¬ì¶•
class CancerNN(nn.Module):
    def __init__(self, input_size=30):
        super(CancerNN, self).__init__()
        self.fc1 = nn.Linear(input_size, 64)
        self.fc2 = nn.Linear(64, 32)
        self.fc3 = nn.Linear(32, 16)
        self.fc4 = nn.Linear(16, 2)  # 2ê°œ í´ë˜ìŠ¤: ì•…ì„±/ì–‘ì„±
        self.dropout = nn.Dropout(0.3)
        
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.dropout(x)
        x = torch.relu(self.fc2(x))
        x = self.dropout(x)
        x = torch.relu(self.fc3(x))
        x = self.fc4(x)
        return x

# ëª¨ë¸ ì´ˆê¸°í™”
nn_model = CancerNN()
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(nn_model.parameters(), lr=0.001)

# ì‹ ê²½ë§ í•™ìŠµ
print("ì‹ ê²½ë§ í•™ìŠµ ì¤‘...")
nn_model.train()
for epoch in range(100):
    optimizer.zero_grad()
    outputs = nn_model(X_train_tensor)
    loss = criterion(outputs, y_train_tensor)
    loss.backward()
    optimizer.step()
    
    if (epoch + 1) % 20 == 0:
        print(f'ì—í¬í¬ [{epoch+1}/100], ì†ì‹¤: {loss.item():.4f}')

# ì‹ ê²½ë§ í‰ê°€
nn_model.eval()
with torch.no_grad():
    outputs = nn_model(X_test_tensor)
    _, predicted = torch.max(outputs.data, 1)
    nn_accuracy = accuracy_score(y_test, predicted.numpy())
    nn_probabilities = torch.softmax(outputs, dim=1)[:, 1].numpy()
    nn_auc = roc_auc_score(y_test, nn_probabilities)
    
    results['NeuralNetwork'] = {
        'accuracy': nn_accuracy,
        'auc': nn_auc,
        'predictions': predicted.numpy(),
        'probabilities': nn_probabilities
    }
    
    print(f"ì‹ ê²½ë§ ì •í™•ë„: {nn_accuracy:.3f}")
    print(f"ì‹ ê²½ë§ AUC: {nn_auc:.3f}")

# -----------------------
# ë¹„ì§€ë„í•™ìŠµ: KMeans êµ°ì§‘í™”
# -----------------------
print("\nğŸ“˜ ë¹„ì§€ë„í•™ìŠµ - ì•” ë°ì´í„° êµ°ì§‘í™”")
kmeans = KMeans(n_clusters=2, random_state=42)
clusters = kmeans.fit_predict(X_scaled)

# ì‹¤ì œ ë¼ë²¨ê³¼ ë¹„êµí•˜ì—¬ êµ°ì§‘í™” í‰ê°€
cluster_accuracy = max(
    accuracy_score(y, clusters),
    accuracy_score(y, 1 - clusters)  # ì—­ë§¤í•‘ ì‹œë„
)
print(f"KMeans êµ°ì§‘í™” ì •í™•ë„: {cluster_accuracy:.3f}")

# -----------------------
# ì‹œê°í™”: ê²°ê³¼ ë¶„ì„
# -----------------------
print("\nğŸ“˜ ì‹œê°í™” - ê²°ê³¼ ë¶„ì„")

# 1. íŠ¹ì„± ì¤‘ìš”ë„ (ëœë¤ í¬ë ˆìŠ¤íŠ¸)
rf_model = models['RandomForest']
feature_importance = pd.DataFrame({
    'feature': cancer.feature_names,
    'importance': rf_model.feature_importances_
}).sort_values('importance', ascending=False)

plt.figure(figsize=(15, 10))

# íŠ¹ì„± ì¤‘ìš”ë„ ê·¸ë˜í”„
plt.subplot(2, 3, 1)
top_features = feature_importance.head(10)
plt.barh(range(len(top_features)), top_features['importance'])
plt.yticks(range(len(top_features)), top_features['feature'])
plt.xlabel('íŠ¹ì„± ì¤‘ìš”ë„')
plt.title('ìƒìœ„ 10ê°œ ì¤‘ìš” íŠ¹ì„± (ëœë¤ í¬ë ˆìŠ¤íŠ¸)')
plt.gca().invert_yaxis()

# í˜¼ë™í–‰ë ¬
for i, (name, result) in enumerate(results.items()):
    plt.subplot(2, 3, i + 2)
    cm = confusion_matrix(y_test, result['predictions'])
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f'{name} í˜¼ë™í–‰ë ¬')
    plt.xlabel('ì˜ˆì¸¡ê°’')
    plt.ylabel('ì‹¤ì œê°’')

plt.tight_layout()
plt.show()

# 2. ROC ê³¡ì„ 
plt.figure(figsize=(10, 6))
for name, result in results.items():
    if result['probabilities'] is not None:
        fpr, tpr, _ = roc_curve(y_test, result['probabilities'])
        plt.plot(fpr, tpr, label=f'{name} (AUC = {result["auc"]:.3f})')

plt.plot([0, 1], [0, 1], 'k--', label='ë¬´ì‘ìœ„')
plt.xlabel('ê±°ì§“ ì–‘ì„±ë¥ ')
plt.ylabel('ì§„ì§œ ì–‘ì„±ë¥ ')
plt.title('ROC ê³¡ì„  ë¹„êµ')
plt.legend()
plt.grid(True)
plt.show()

# 3. êµ°ì§‘í™” ì‹œê°í™” (2Dë¥¼ ìœ„í•œ PCA)
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

plt.figure(figsize=(12, 4))

# ì‹¤ì œ ë¼ë²¨
plt.subplot(1, 2, 1)
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', alpha=0.6)
plt.colorbar(scatter)
plt.title('ì‹¤ì œ ë¼ë²¨ (ì•…ì„± vs ì–‘ì„±)')
plt.xlabel('PCA ì„±ë¶„ 1')
plt.ylabel('PCA ì„±ë¶„ 2')

# êµ°ì§‘í™” ê²°ê³¼
plt.subplot(1, 2, 2)
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=clusters, cmap='viridis', alpha=0.6)
plt.colorbar(scatter)
plt.title('KMeans êµ°ì§‘í™” ê²°ê³¼')
plt.xlabel('PCA ì„±ë¶„ 1')
plt.ylabel('PCA ì„±ë¶„ 2')

plt.tight_layout()
plt.show()

# -----------------------
# ëª¨ë¸ ë¹„êµ ë° ì˜ë£Œì  í†µì°°
# -----------------------
print("\nğŸ“˜ ëª¨ë¸ ë¹„êµ ë° ì˜ë£Œì  í†µì°°")
print("=" * 50)
print("ëª¨ë¸ ì„±ëŠ¥ ìš”ì•½:")
print("-" * 30)
for name, result in results.items():
    print(f"{name:15} | ì •í™•ë„: {result['accuracy']:.3f} | AUC: {result['auc']:.3f}")

print(f"\nKMeans êµ°ì§‘í™” | ì •í™•ë„: {cluster_accuracy:.3f}")
print("=" * 50)

# ì˜ë£Œì  í•´ì„
print("\nğŸ“˜ ì˜ë£Œì  í•´ì„:")
print("- ì–‘ì„± (0): ì–‘ì„± ì¢…ì–‘ - ì¼ë°˜ì ìœ¼ë¡œ ìœ„í—˜í•˜ì§€ ì•ŠìŒ")
print("- ì•…ì„± (1): ì•…ì„± ì¢…ì–‘ - ì•”ìœ¼ë¡œ ë¶„ë¥˜ë˜ì–´ ì¦‰ì‹œ ì¹˜ë£Œ í•„ìš”")
print("- ë†’ì€ ì •í™•ë„ëŠ” ì˜ë£Œ ì§„ë‹¨ì— ë§¤ìš° ì¤‘ìš”")
print("- ê±°ì§“ ìŒì„±(ì•”ì„ ë†“ì¹œ ê²½ìš°)ì´ ê±°ì§“ ì–‘ì„±ë³´ë‹¤ ë” ìœ„í—˜")

# íŠ¹ì„± í†µì°°
print(f"\nğŸ“˜ ì£¼ìš” ì§„ë‹¨ íŠ¹ì„± (ìƒìœ„ 5ê°œ):")
for i, (_, row) in enumerate(feature_importance.head().iterrows()):
    print(f"{i+1}. {row['feature']} (ì¤‘ìš”ë„: {row['importance']:.3f})")

print("\nğŸ“˜ ì„ìƒì  ê¶Œì¥ì‚¬í•­:")
print("- êµì°¨ ê²€ì¦ì„ ìœ„í•´ ì—¬ëŸ¬ ëª¨ë¸ ì‚¬ìš© ê¶Œì¥")
print("- ê³ ìœ„í—˜ í™˜ìì˜ ì •ê¸° ê²€ì§„ ë° í›„ì† ì¡°ì¹˜")
print("- í™˜ì ë³‘ë ¥ ë° ì¶”ê°€ ê²€ì‚¬ ê³ ë ¤")
print("- AIëŠ” ì˜ë£Œì§„ì„ ë³´ì¡°í•˜ëŠ” ë„êµ¬ì´ì§€ ëŒ€ì²´í•  ìˆ˜ ì—†ìŒ") 